---
title: 'A Simple OpenAI Function Calls Framework'
description: 'Creating a simple framework for developing and testing OpenAI Function Calls'
pubDate: 'Mar 09 2025'
heroImage: '../../assets/images/palmtrees.jpg'
category: 'Tech'
tags: ['AI', 'LLM', 'Python']
---

# Testing OpenAI Function Calls with a Simple Framework

I've been experimenting with OpenAI's function calling feature and wanted a way to test my own function calls in a simple and structured manner. I aimed for a lightweight framework that would allow me to define tools and functions separately while letting my chatbot intelligently decide when to call them.  

To make the setup portable and efficient, I decided to build everything around a container image. This approach ensures that:  
- The entire system can be easily spun up in any environment.  
- The actual code remains outside the container, avoiding unnecessary rebuilds during development.  

## The Idea

OpenAI's API platform has its own Playground where you can test function calling, but I wanted a way to be able to test my own local code. The goal was to have a chatbot that:

1. Receives user messages via a web interface.  
2. Sends the messages to OpenAI's API, which decides if a function needs to be called.  
3. Calls the relevant function if needed and returns the result.  
4. Displays everything in a simple front-end.  

To keep things modular, I placed function definitions in a `functions/` folder and tool schemas in a `tools/` folder. This way, adding new functionality is just a matter of dropping in a new Python file.  
## Project Structure

```
app/
│── frontend/
│   └── index.html
│── functions/
│   ├── get_weather.py
│── tools/
│   ├── get_weather_tool.py
│── main.py
│── requirements.txt
└── Dockerfile
```

## Step-by-Step: Understanding `main.py`

The core of the chatbot lives in `main.py`. Let's break it down step by step.

### 1. Setting Up Flask and OpenAI

```python
import os
import json
from flask import Flask, request, jsonify, send_from_directory
from openai import OpenAI
import importlib

app = Flask(__name__)
client = OpenAI()
```

Here, we initialize Flask for our web server and create an OpenAI client to handle API requests.

### 2. Loading Functions and Tools Dynamically

```python
# Directories for tools and functions
TOOLS_DIR = os.path.join(os.path.dirname(__file__), "tools")
FUNCTIONS_DIR = os.path.join(os.path.dirname(__file__), "functions")

def load_modules(directory):
    modules = {}
    for filename in os.listdir(directory):
        if filename.endswith('.py') and filename != '__init__.py':
            module_name = f"{directory.split('/')[-1]}.{filename[:-3]}"
            module = importlib.import_module(module_name)
            for attr in dir(module):
                obj = getattr(module, attr)
                if callable(obj):
                    modules[obj.__name__] = obj
    return modules

# Load functions and tools
functions = load_modules(FUNCTIONS_DIR)
tools_definitions = load_modules(TOOLS_DIR)

tools = []
for tool_name, tool in tools_definitions.items():
    tool_schema = tool()  # Assuming each tool script returns a dictionary defining the tool
    tools.append(tool_schema)
```

This dynamically imports all Python scripts inside `functions/` and `tools/`, making it easy to extend the chatbot.


### 3. Example Functions script

Each function is defined in its own script and loaded dynamically.

```python
import requests

def get_weather(location: str) -> str:
    """
    Fetches weather information for the given location.
    
    Args:
        location (str): The name of the city to fetch weather data for.
    
    Returns:
        str: A string describing the weather conditions.
    """
    
    api_url = f"https://wttr.in/{location}?format=%C+%t"
    try:
        response = requests.get(api_url)
        if response.status_code == 200:
            return f"Weather in {location}: {response.text}"
        else:
            return f"Could not retrieve weather for {location}."
    except Exception as e:
        return f"Error fetching weather: {str(e)}"

```


### 4. Example Tools script

Each tool is defined in its own script and loaded dynamically.

```python
def get_weather_tool():
    """
    Defines the tool schema for fetching weather information.
    """
    return {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Retrieve weather information for a specified location.",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "Name of the city to get weather information for."
                    }
                },
                "required": ["location"],
                "additionalProperties": False
            },
            "strict": True
        }
    }
```

### 4. Handling Chat Requests

```python
@app.route("/chat", methods=["POST"])
def chat():
    data = request.get_json()
    user_message = data.get("message", "")

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": user_message}
    ]

    # Call OpenAI's chat completion with function calling
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        tools=tools
    )

    response_message = response.choices[0].message
```

This function receives user input and sends it to OpenAI's API.
If OpenAI decides to call a function, we execute it and append the result to the chat history.
After executing the function, we ask OpenAI for a final response.

### 5. Handling Function Calls

```python
    # Check if the model decided to call a function
    if response_message.tool_calls:
        for tool_call in response_message.tool_calls:
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)
            
            # Execute the corresponding function
            if function_name in functions:
                function_response = functions[function_name](**function_args)
            else:
                function_response = f"Function '{function_name}' is not recognized."

            # Append the function's response to the conversation
            messages.append(response_message)
            messages.append({
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": function_response
            })

        # Get the model's response after function execution
        second_response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages
        )
        final_text = second_response.choices[0].message.content
    else:
        # If no function was called, use the model's direct response
        final_text = response_message.content

    return jsonify({"reply": final_text})
```

### 7. Serving the Frontend

```python
@app.route("/")
def serve_index():
    return send_from_directory(os.path.join(os.path.dirname(__file__), "frontend"), "index.html")
```

This serves the chatbot's front-end where we can test the functions.

### 8. Running the App

```python
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
```

This starts the Flask server, exposing it on port 5000.

## Running the App in Docker

To make deployment easy, I created a `Dockerfile`:

```dockerfile
FROM python:3.9-slim

# Set the working directory in the container
WORKDIR /app

# Copy requirements and install them
COPY app/requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt

# Expose port 5000 for the Flask app
EXPOSE 5000

# Run the Flask app
CMD ["python", "/app/main.py"]

```

## The Frontend

To keep things simple, I made a small HTML page (`frontend/index.html`) where users can chat with the bot:

<details>
  <summary>Click to expand</summary>

  ```html
  <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Local Chatbot</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f4;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      margin: 0;
    }
    .chat-container {
      background: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
      width: 400px;
      text-align: center;
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    h1 {
      font-size: 24px;
      margin-bottom: 10px;
    }
    #chat-log {
      border: 1px solid #ccc;
      width: 100%;
      height: 300px;
      overflow-y: auto;
      padding: 10px;
      background: #fafafa;
      border-radius: 5px;
      display: flex;
      flex-direction: column;
      justify-content: flex-end;
    }
    .input-container {
      display: flex;
      width: 100%;
      margin-top: 10px;
    }
    input {
      flex: 1;
      padding: 10px;
      border: 1px solid #ccc;
      border-radius: 5px;
    }
    button {
      padding: 10px;
      background-color: #007BFF;
      color: white;
      border: none;
      border-radius: 5px;
      cursor: pointer;
      margin-left: 5px;
    }
    button:hover {
      background-color: #0056b3;
    }
  </style>
</head>
<body>
  <div class="chat-container">
    <h1>My Local Chatbot</h1>
    <div id="chat-log"></div>
    <div class="input-container">
      <input type="text" id="user-input" placeholder="Type your message..." onkeypress="handleKeyPress(event)" />
      <button onclick="sendMessage()">Send</button>
    </div>
  </div>

  <script>
    async function sendMessage() {
      const chatLog = document.getElementById('chat-log');
      const userInput = document.getElementById('user-input');
      const message = userInput.value.trim();
      if (!message) return;

      addToChat('User', message);
      userInput.value = "";

      try {
        const response = await fetch("/chat", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ message: message })
        });
        const data = await response.json();
        addToChat('Bot', data.reply);
      } catch (err) {
        addToChat('Bot', "Error: " + err.toString());
      }
    }

    function addToChat(role, text) {
      const chatLog = document.getElementById("chat-log");
      const messageElement = document.createElement("div");
      messageElement.innerHTML = `<strong>${role}:</strong> ${text}`;
      chatLog.appendChild(messageElement);
      chatLog.scrollTop = chatLog.scrollHeight;
    }

    function handleKeyPress(event) {
      if (event.key === "Enter") {
        sendMessage();
      }
    }
  </script>
</body>
</html>
```
</details>


Build and run the container:

```sh
docker build -t my-chatbot .
docker run -p 5000:5000 e OPENAI_API_KEY=<YOUR_OPENAI_API_KEY> -v $(pwd)/app:/app my-chatbot
```


## Conclusion

This setup lets me quickly test new function calls, and the modular structure makes it easy to add new functions and tools. If I want to extend it, I can just drop in new `.py` files in `functions/` and `tools/`, and the chatbot will automatically pick them up.
